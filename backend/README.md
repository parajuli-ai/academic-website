# Academic RAG Backend API

Production-ready RAG (Retrieval-Augmented Generation) backend for academic personal website with document upload and intelligent Q&A capabilities.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI App   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
â”‚ LLM  â”‚  â”‚Vector â”‚
â”‚Geminiâ”‚  â”‚ Store â”‚
â””â”€â”€â”€â”€â”€â”€â”˜  â”‚Pinecone
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

- **FastAPI**: Modern async web framework
- **Google Gemini**: LLM for answer generation
- **Pinecone**: Vector database for semantic search
- **Document Processor**: PDF/TXT/MD/DOCX support with intelligent chunking

## ğŸš€ Features

- âœ… Document upload (PDF, TXT, MD, DOCX)
- âœ… Intelligent text chunking with overlap
- âœ… Semantic search with vector embeddings
- âœ… Context-aware answer generation
- âœ… Conversation history tracking
- âœ… CORS support for web integration
- âœ… Comprehensive error handling
- âœ… Health monitoring
- âœ… Production-ready logging

## ğŸ“‹ Prerequisites

- Python 3.11+
- Google AI API key ([Get here](https://ai.google.dev/))
- Pinecone API key ([Get here](https://www.pinecone.io/))

## ğŸ› ï¸ Installation

### Local Development

1. **Clone and navigate to backend**:
```bash
cd backend
```

2. **Create virtual environment**:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

4. **Configure environment**:
```bash
cp env.example .env
# Edit .env with your API keys
```

5. **Run the server**:
```bash
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

API will be available at `http://localhost:8000`

### Docker Deployment

1. **Build and run**:
```bash
docker-compose up --build
```

2. **Access API**:
- API: `http://localhost:8000`
- Docs: `http://localhost:8000/docs`

## ğŸ“¡ API Endpoints

### Health Check
```http
GET /health
```

Response:
```json
{
  "status": "healthy",
  "version": "1.0.0",
  "services": {
    "vector_store": "healthy",
    "llm": "healthy"
  }
}
```

### Upload Document
```http
POST /upload
Content-Type: multipart/form-data

file: <your_file.pdf>
```

Response:
```json
{
  "document_id": "abc123",
  "filename": "cv.pdf",
  "status": "completed",
  "chunks_created": 15,
  "message": "Document processed successfully"
}
```

### Chat (RAG Query)
```http
POST /chat
Content-Type: application/json

{
  "query": "What is Tilak's research focus?",
  "conversation_id": "conv_123" // optional
}
```

Response:
```json
{
  "answer": "Tilak's research focuses on...",
  "sources": [
    {
      "text": "Relevant context...",
      "score": 0.89,
      "metadata": {"filename": "cv.pdf"}
    }
  ],
  "conversation_id": "conv_123",
  "confidence": 0.85
}
```

### List Documents
```http
GET /documents
```

### Delete Document
```http
DELETE /documents/{document_id}
```

## ğŸŒ Vercel Deployment

1. **Install Vercel CLI**:
```bash
npm i -g vercel
```

2. **Set environment variables** in Vercel dashboard:
   - `GOOGLE_API_KEY`
   - `PINECONE_API_KEY`
   - `PINECONE_INDEX_NAME`

3. **Deploy**:
```bash
vercel --prod
```

## ğŸ”§ Configuration

Edit `config.py` or set environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `GOOGLE_API_KEY` | Google AI API key | Required |
| `PINECONE_API_KEY` | Pinecone API key | Required |
| `LLM_MODEL` | Gemini model | `gemini-1.5-flash` |
| `EMBED_MODEL` | Embedding model | `text-embedding-004` |
| `CHUNK_SIZE` | Text chunk size | `1000` |
| `CHUNK_OVERLAP` | Chunk overlap | `200` |
| `TOP_K_RESULTS` | Retrieval results | `5` |
| `SIMILARITY_THRESHOLD` | Min similarity | `0.7` |

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ app.py                 # FastAPI application
â”œâ”€â”€ config.py              # Configuration management
â”œâ”€â”€ models.py              # Pydantic models
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ document_processor.py  # Document parsing & chunking
â”‚   â”œâ”€â”€ vector_store.py        # Pinecone integration
â”‚   â””â”€â”€ llm_service.py         # Gemini LLM service
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py          # Logging configuration
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile             # Docker configuration
â”œâ”€â”€ docker-compose.yml     # Docker Compose
â”œâ”€â”€ vercel.json            # Vercel deployment
â””â”€â”€ README.md              # This file
```

## ğŸ§ª Testing

### Manual Testing

1. **Upload a document**:
```bash
curl -X POST "http://localhost:8000/upload" \
  -F "file=@/path/to/document.pdf"
```

2. **Query the system**:
```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"query": "What are the main projects?"}'
```

### API Documentation

Interactive API docs available at:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

## ğŸ”’ Security Considerations

- âœ… CORS configured for specific origins
- âœ… File size limits enforced
- âœ… Input validation with Pydantic
- âœ… Error messages sanitized in production
- âœ… Non-root Docker user
- âš ï¸ Add rate limiting for production (e.g., slowapi)
- âš ï¸ Implement authentication for upload endpoint
- âš ï¸ Use secrets manager for API keys

## ğŸ“Š Monitoring

- Health endpoint: `/health`
- Structured logging with timestamps
- Service status checks for all components

## ğŸ› Troubleshooting

### Common Issues

1. **"Service unavailable" errors**:
   - Check API keys are set correctly
   - Verify Pinecone index exists
   - Check network connectivity

2. **Embedding dimension mismatch**:
   - Ensure `EMBED_DIMENSION` matches your model
   - `text-embedding-004` uses 768 dimensions

3. **File upload fails**:
   - Check file size < `MAX_FILE_SIZE_MB`
   - Verify file extension is supported
   - Ensure sufficient memory

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“§ Support

For issues or questions, please open an issue on GitHub.

---

Built with â¤ï¸ using FastAPI, Google Gemini, and Pinecone
